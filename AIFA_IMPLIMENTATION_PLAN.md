A is for Agent CLI (AIFA)

AIFA is a terminal-first, vendor-agnostic AI agent that keeps all the power-user ergonomics of modern dev CLIs while removing cloud lock-in. It runs fully local by default, speaking an OpenAI-compatible API so it can drive models served by LM Studio (or any compatible server), and it preserves a rich tool ecosystem (filesystem, shell, web fetch/search, multi-file reads, memory, sandboxing).

Why this fork exists
	‚Ä¢	Local-first: Use powerful LLMs on your own machine for privacy, speed, and cost control.
	‚Ä¢	No Google auth: All Google/Gemini authentication flows are removed; AIFA talks to a local OpenAI-style endpoint instead.
	‚Ä¢	Same great UX: Keep the TUI, tools, confirmations, and sandboxing you‚Äôre used to‚Äîjust swap the model backend.
	‚Ä¢	TDD from the ground up: Every feature and refactor lands with tests (unit, integration, and mocked end-to-end).

What AIFA does
	‚Ä¢	Chat and agent workflows with function/tool calling.
	‚Ä¢	Streaming outputs (text deltas) with robust streamed tool-call assembly (multi-tool aware).
	‚Ä¢	Rich built-in tools: read/write files, run shell commands (guarded), web fetch/search, read many files, memory notes.
	‚Ä¢	Sandboxing options (Seatbelt/containers) to keep tool execution contained.
	‚Ä¢	Configurable via project/user settings, env vars, and flags.

What changed vs. upstream
	‚Ä¢	‚úÇÔ∏è Removed: Google/Gemini auth and direct Gemini API calls.
	‚Ä¢	üîå Added: OpenAI-compatible provider targeting http://localhost:1234/v1 by default (LM Studio).
	‚Ä¢	üß† Added: Streamed tool_call accumulation (handles partial JSON over SSE).
	‚Ä¢	ü©∫ Added: First-run health checks (/v1/models) and a /models command for discoverability.
	‚Ä¢	üõ°Ô∏è Added: Better error surfacing and auto-truncation of large tool results to protect context.
	‚Ä¢	ü™∂ Optional: Fetch-only transport (no SDK) for lighter builds.
	‚Ä¢	üëÅÔ∏è Optional: Vision/content mapping to OpenAI content arrays, with graceful degradation.

Audience

Developers, power users, and ops folks who want a fast, local, scriptable agent with strong ergonomics, minimal external dependencies, and clear security boundaries.

Design principles
	‚Ä¢	Local by default; remote optional.
	‚Ä¢	Small surface, sharp edges: explicit confirmations for sensitive tools.
	‚Ä¢	Predictable config: env ‚Üí project settings ‚Üí user settings ‚Üí flags (documented precedence).
	‚Ä¢	TDD discipline: red ‚Üí green ‚Üí refactor; tests are non-negotiable.

Non-goals
	‚Ä¢	Cloud account management or remote auth flows.
	‚Ä¢	Feature parity with every proprietary multimodal feature; AIFA favors pragmatic, portable mappings.

Quick start (local)

# Start LM Studio and enable the OpenAI server on port 1234 with a loaded model.
export OPENAI_API_BASE=http://localhost:1234/v1
export OPENAI_API_KEY=lm-studio       # any non-empty string works for LM Studio
export OPENAI_MODEL=<your-model-id>   # e.g., qwen2.5-coder

# Run the CLI (after building/instaling AIFA)
aifa -p "Hello, AIFA!"

Naming & files: This fork uses AIFA throughout. Docs and helper files use AIFA.md (not GEMINI.md), and commands/help text reference ‚ÄúA is for Agent CLI (AIFA)‚Äù.

‚∏ª

Epic: A is for Agent CLI - Vendor-agnostic LLM core + Ollama\LM Studio (OpenAI-compatible) backend

High-level milestones
	1.	Baseline & safety net
	2.	Provider abstraction (no behavior change)
	3.	OpenAI-compatible provider (SDK + optional fetch transport)
	4.	Streaming tool-call handling (critical)
	5.	Health checks, API key enforcement, /models command
	6.	Error handling + tool result truncation
	7.	Remove Google auth; CLI polish
	8.	Vision/content mapping (optional)
	9.	E2E coverage (mocked) + docs & release

‚∏ª

0) Baseline & project switches

Branching
	‚Ä¢	feat/provider-abstraction ‚Üí feat/openai-compatible ‚Üí feat/stream-toolcalls ‚Üí feat/health-and-models ‚Üí feat/errors-and-truncation ‚Üí feat/remove-google-auth ‚Üí feat/vision-mapping ‚Üí qa/e2e.

Ground rules
	‚Ä¢	Tests first (red ‚Üí green ‚Üí refactor).
	‚Ä¢	Keep commits small and reversible.
	‚Ä¢	Gate every milestone with acceptance criteria.

Initial tasks
	‚Ä¢	Fork/clone.
	‚Ä¢	Run the repo‚Äôs existing tests; add two smoke tests for the CLI:
	‚Ä¢	REPL banner snapshot.
	‚Ä¢	/help snapshot.

‚∏ª

1) Provider abstraction (no behavior change)

Goal: decouple Core from any vendor.

Code
	‚Ä¢	Add packages/core/src/llm/LlmProvider.ts with a simple chat() contract (supports messages, tools, streaming hooks).
	‚Ä¢	Implement GoogleGeminiProvider adapter that wraps the current implementation (temporary).
	‚Ä¢	In Core, replace direct calls to Gemini with LlmProvider.chat().

Tests (write first)
	‚Ä¢	Unit: Core tool loop using a FakeProvider that:
	‚Ä¢	turn 1 ‚Üí returns a toolCall; after tool response is appended,
	‚Ä¢	turn 2 ‚Üí returns final text.
	‚Ä¢	Unit: GoogleGeminiProvider adapter returns the unified { text, toolCalls } shape (mock Gemini transport).

Done
	‚Ä¢	All existing behavior intact; green tests.

‚∏ª

2) OpenAI-compatible provider (LM Studio)

Goal: talk to http://localhost:1234/v1 with OpenAI-style API.

Code
	‚Ä¢	OpenAICompatibleProvider (SDK transport). Env:
	‚Ä¢	OPENAI_API_BASE=http://localhost:1234/v1 (default),
	‚Ä¢	OPENAI_API_KEY=lm-studio (required),
	‚Ä¢	OPENAI_MODEL=<model-id>.
	‚Ä¢	providerFactory chooses provider by LLM_PROVIDER=openai-compatible (default).

Tests (first)
	‚Ä¢	Unit: builds correct messages[], tools[], tool_choice and sends headers (Authorization).
	‚Ä¢	Unit: parses non-stream response (message.content, message.tool_calls).

Done
	‚Ä¢	Core can switch providers via env; no CLI changes yet.

‚∏ª

3) Critical: full streaming tool-call handling

Goal: handle text deltas and streamed tool_call fragments.

Code
	‚Ä¢	In OpenAICompatibleProvider, for streaming:
	‚Ä¢	Maintain textBuf.
	‚Ä¢	Maintain a per-index accumulator { id?, name?, argsSrc }.
	‚Ä¢	For each SSE chunk:
	‚Ä¢	append delta.content ‚Üí emit onDelta({text}).
	‚Ä¢	append delta.tool_calls[n].function.arguments to that index‚Äôs argsSrc; update id/name if present.
	‚Ä¢	On EOS: parse each argsSrc into JSON; return { text, toolCalls } via onDone.

Core
	‚Ä¢	If stream ends with toolCalls, stop rendering, execute tools, append tool messages, then call provider again for the final assistant message.

Tests (first)
	‚Ä¢	Unit: interleaved text + two tool calls streamed across many chunks ‚Üí finalizes both with correct JSON.
	‚Ä¢	Unit: ensures order by tool index.

Done
	‚Ä¢	Multi-tool streaming works without ‚Äúincomplete JSON‚Äù errors.

‚∏ª

4) High: health checks, API key enforcement, /models

Goal: great first-run UX.

Code
	‚Ä¢	Enforce OPENAI_API_KEY at provider init (dummy values allowed).
	‚Ä¢	Add listModels() in provider (GET /v1/models).
	‚Ä¢	CLI bootstrap: on first run (unless --no-check), call listModels():
	‚Ä¢	If none found ‚Üí print friendly guide (‚ÄúStart LM Studio ‚Üí Developer ‚Üí Start Server; load a model‚Äù).
	‚Ä¢	Show top 5 models and highlight OPENAI_MODEL if set.
	‚Ä¢	Add /models command to list models; optionally /model set <id> to persist.

Tests (first)
	‚Ä¢	Unit: models 200 ‚Üí IDs returned; 401/404/500 ‚Üí helpful errors.
	‚Ä¢	CLI: /models snapshot; /model set updates setting.

Done
	‚Ä¢	Users can discover and select models easily; missing key/server is obvious.

‚∏ª

5) High: error handling & truncation

Goal: resilience against malformed tool args, big tool outputs, server hiccups.

Code
	‚Ä¢	safeParseJson() with light ‚Äúbrace-balancing‚Äù repair and enriched error messages (include a truncated source preview).
	‚Ä¢	Tool result truncation before sending back to the model:
	‚Ä¢	TOOL_RESULT_TOKEN_LIMIT=1000 (‚âà char heuristic *4).
	‚Ä¢	TOOL_RESULT_CHAR_LIMIT=4000.
	‚Ä¢	Annotate: "[...truncated N chars]".
	‚Ä¢	Uniform LlmProviderError carrying status, endpoint, and a tip (e.g., ‚ÄúEnsure a model is loaded‚Äù).

Tests (first)
	‚Ä¢	Bad JSON fragments ‚Üí repaired ‚Üí OK.
	‚Ä¢	Still-bad JSON ‚Üí throws with preview.
	‚Ä¢	Huge tool output ‚Üí truncated and annotated.
	‚Ä¢	Non-2xx from /chat ‚Üí error includes status + tip.

Done
	‚Ä¢	No crashes; context stays bounded.

‚∏ª

6) Medium: optional fetch transport (lighter builds)

Goal: no SDK dependency if desired.

Code
	‚Ä¢	OpenAICompatibleFetchProvider with fetch + manual SSE line parsing (\n\n framed data: lines).
	‚Ä¢	Env: LLM_TRANSPORT=fetch|sdk (default sdk).
	‚Ä¢	Reuse the same accumulation code as in SDK path.

Tests (first)
	‚Ä¢	Parity: request body (messages/tools/tool_choice) matches SDK snapshot.
	‚Ä¢	Streaming: multi-tool case finalizes correctly.

Done
	‚Ä¢	Either transport passes the same suite.

‚∏ª

7) Medium: remove Google auth; CLI polish

Goal: de-Google the UX while keeping all features.

Code
	‚Ä¢	Remove OAuth/Vertex/API-key Gemini code & deps.
	‚Ä¢	/auth command:
	‚Ä¢	Either remove from /help, or keep as a no-op that prints:
‚ÄúThis fork uses a local OpenAI-compatible server. Set OPENAI_API_KEY and (optionally) OPENAI_MODEL. Try /models.‚Äù
	‚Ä¢	First-run banner: show base URL, key presence, top 5 models.

Tests (first)
	‚Ä¢	CLI help no longer lists Google auth commands (or shows no-op text).

Done
	‚Ä¢	No Google references remain; first-run is self-explanatory.

‚∏ª

8) Low: vision/content ‚Äúparts‚Äù mapping (optional)

Goal: handle images when available; degrade gracefully otherwise.

Code
	‚Ä¢	Map internal ‚Äúparts‚Äù ‚Üí OpenAI content arrays:
{type:"text",text} and {type:"image_url", image_url:{url: data:<mime>;base64,<...>}}.
	‚Ä¢	If model/server rejects multimodal, print a gentle hint and fall back to text-only behavior.

Tests (first)
	‚Ä¢	Parts mapping unit tests; degrade-to-text snapshot.

Done
	‚Ä¢	Vision works where models support it; otherwise harmless fallback.

‚∏ª

9) Broad E2E (mocked) + docs & release

Goal: high confidence without requiring a live server in CI.

Code/Tests
	‚Ä¢	Use MSW (or similar) to mock:
	‚Ä¢	GET /v1/models (happy + error).
	‚Ä¢	POST /v1/chat/completions (non-stream & stream fixtures, incl. multi-tool).
	‚Ä¢	E2E scenarios:
	‚Ä¢	‚ÄúList files then count them‚Äù ‚Üí two tool calls ‚Üí final assistant summary.
	‚Ä¢	Abort mid-stream (Ctrl-C) ‚Üí clean cancellation via AbortController.
	‚Ä¢	Parameter passthrough (temperature/top-p) snapshot.

Docs
	‚Ä¢	README quick start (2 steps): Start LM Studio server ‚Üí run CLI with env.
	‚Ä¢	Troubleshooting table (401/404, ‚Äúmodel not loaded‚Äù, malformed stream).
	‚Ä¢	Config matrix (env vars, flags, settings.json precedence).

Done
	‚Ä¢	CI green; docs ready; tagged release of your fork.

‚∏ª

Configuration quick sheet

export LLM_PROVIDER=openai-compatible
export LLM_TRANSPORT=sdk            # or fetch
export OPENAI_API_BASE=http://localhost:1234/v1
export OPENAI_API_KEY=lm-studio     # any non-empty string is fine
export OPENAI_MODEL=qwen2.5-coder   # replace with a loaded model id

# Optional safeguards
export TOOL_RESULT_TOKEN_LIMIT=1000
export TOOL_RESULT_CHAR_LIMIT=4000


‚∏ª

Acceptance checklist (copy/paste)
	‚Ä¢	CLI starts without Google auth prompts; health check runs; shows model list.
	‚Ä¢	/models works; /model set <id> (if implemented) persists choice.
	‚Ä¢	Streaming with two+ tool calls completes (no partial JSON).
	‚Ä¢	Oversized tool output is truncated and annotated.
	‚Ä¢	SDK and fetch transports pass the same tests.
	‚Ä¢	Vision parts map or degrade gracefully.
	‚Ä¢	E2E: chat + tools + stream + abort + error branches covered.
	‚Ä¢	Docs: quick start + troubleshooting + config precedence.